"""
Attempting to implement GANs, first for simple 1d probability distributions
and then ideally for MNIST.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import os

# trying out abstract base class stuff
from collections.abs import abstractmethod

class GenerativeAdversarialNetwork(object):
    """
    This is a class to house a Generator and a Discriminator and train them jointly.
    I'm still not sure the best way to pass stuff in here but I'll work it out
    """

class Discriminator(nn.Module):
    """
    This is a general template for discriminators.
    I don't have a lot of experience with this type of programming, but the idea is
    that all discriminators that I use will inherit from this, and this will have abstract methods.
    TODO: figure out if I should restructure this to CONTAIN an nn.Module rather than be one.
    """

    def __init__(self):
        """
        Constructor for Discriminator networks.
        Saving borrowed from RL CW again.

        :attr output_dim (int): Disciminator is always a binary classifier, so this is always 1
        :attr saveables(Dict[str, torch.nn.Module]): stuff to save
        """
        # initialise as a pytorch module
        super(Discriminator, self).__init__()

        self.output_dim = 1
        self.saveables = {}

    def compute_loss(self, train_mb, gen_mb):
        """Compute the loss for a minibatch.
        
        :param train_mb (torch.Tensor): minibatch of data from the training distribution, size (batch_size, (data_dims))
        :param gen_mb (torch.Tensor): minibatch of data generated by the generator, size (batch_size, (data_dims))
        :return loss (torch.Tensor): The scalar loss on this minibatch, from which to compute gradients
        """
        # make discriminator predictions for the two minibatches
        train_preds = self(train_mb)
        gen_preds = self(gen_mb)
        # return the loss
        return -torch.mean(torch.log(train_preds) + torch.log(1 - gen_preds))

    # borrowed from RL CW
    def save(self, path):
        """Saves saveable PyTorch models under given path

        The models will be saved in directory found under given path in file "models.pt"

        :param path (str): path to directory where to save models
        :return (str): path to file of saved models file
        """
        torch.save(self.saveables, path)
        return path

    # borrowed from RL CW
    def restore(self, save_path):
        """Restores PyTorch models from models file given by path

        :param save_path (str): path to file containing saved models
        """
        dirname, _ = os.path.split(os.path.abspath(__file__))
        save_path = os.path.join(dirname, save_path)
        checkpoint = torch.load(save_path)
        for k, v in self.saveables.items():
            v.load_state_dict(checkpoint[k].state_dict())

    @abstractmethod
    def __call__(self):
        """Need to be able to call the discriminator to make predictions"""
        ...

    @abstractmethod
    def update(self):
        """
        Every discriminator must have a way to update its parameters
        """
        ...


class MLPDiscriminator(Discriminator):
    """
    This class is for an MLP discriminator (feed-forward neural network).
    It inherits from the Disciminator base class
    """
    def __init__(self, input_dim=1, hidden_dim=10, n_hidden_layers=2, lr=1e-3
                 ):
        # initialise the module
        super(Discriminator, self).__init__()

        self.n_hidden_layers = n_hidden_layers
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # store the layers
        self.hiddens = nn.ModuleList()
        self.batchnorms = nn.ModuleList()
        self.build_module()

        # build an optimiser for this network's params
        self.optim = torch.optim.Adam(params=self.parameters(), lr=lr)

    def build_module(self):
        # simulate a minibatch of size 2 being passed through the network
        x = torch.zeros((2, self.input_dim))
        out = x
        self.input_layer = nn.Linear(self.input_dim, self.hidden_dim)
        out = self.input_layer(out)
        # we subtract one because the last hidden layer goes to the output
        for i in range(self.n_hidden_layers - 1):
            # calculate the dimension of the batchnorm needed
            batch_dim = out.size(-1)
            self.batchnorms.append(nn.BatchNorm1d(batch_dim))
            out = self.batchnorms[i](out)
            self.hiddens.append(nn.Linear(self.hidden_dim, self.hidden_dim))
            out = self.hiddens[i](out)
        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)

    def forward(self, X):
        out = X
        out = self.input_layer(out)
        out = F.relu(out)
        for i in range(self.n_hidden_layers - 1):
            out = self.batchnorms[i](out)
            out = self.hiddens[i](out)
            out = F.relu(out)
        # on the last output, we apply a sigmoid to get a binary prediction
        out = self.output_layer(out)
        out = torch.sigmoid(out)
        return out

    def update(self, train_mb, gen_mb):
        """
        Update the network's parameters given a minibatch of training examples and generator samples

        :param train_mb (torch.Tensor): minibatch of data from the training distribution, size (batch_size, (data_dims))
        :param gen_mb (torch.Tensor): minibatch of data generated by the generator, size (batch_size, (data_dims))
        :return loss (torch.Tensor): The scalar loss on this minibatch
        """

        # first we zero the gradients, then compute the loss, backpropagate, and take a step 
        self.optim.zero_grad()
        loss = self.compute_loss(train_mb, gen_mb)
        loss.backward()
        self.optim.step()
        return loss

class Generator(nn.Module):
    """
    This is a general template for generators.
    I don't have a lot of experience with this type of programming, but the idea is
    that all generators that I use will inherit from this, and this will have abstract methods.
    TODO: figure out if I should restructure this to CONTAIN an nn.Module rather than be one.
    """
    def __init__(self, noise_prior, input_dim, output_dim):
        """
        Constructor for Generator networks.
        Saving borrowed from RL CW again.

        :attr input_dim: the size of the noise to pass to the generator
        :attr output_dim: the size to output, i.e. the dimensions of the training data
        :attr noise_prior (torch.distributions): An (instantiated) torch distribution to sample noise from
        :attr saveables (Dict[str, torch.nn.Module]): stuff to save
        """
        # initialise as a pytorch module
        super(Generator, self).__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.noise_prior = noise_prior

        self.saveables = {}

    def compute_loss(self, gen_mb, discriminator):
        """Compute the loss for a minibatch.
        
        :param gen_mb (torch.Tensor): minibatch of data generated by the generator, size (batch_size, (data_dims))
        :param Discriminator (Discriminator): a Discriminator subclass to make predictions
        :return loss (torch.Tensor): The scalar loss on this minibatch, from which to compute gradients
        """
        # make discriminator predictions for the two minibatches
        gen_preds = discriminator(gen_mb)
        # return the loss
        return -torch.mean(torch.log(gen_preds))

    # borrowed from RL CW
    def save(self, path):
        """Saves saveable PyTorch models under given path

        The models will be saved in directory found under given path in file "models.pt"

        :param path (str): path to directory where to save models
        :return (str): path to file of saved models file
        """
        torch.save(self.saveables, path)
        return path

    # borrowed from RL CW
    def restore(self, save_path):
        """Restores PyTorch models from models file given by path

        :param save_path (str): path to file containing saved models
        """
        dirname, _ = os.path.split(os.path.abspath(__file__))
        save_path = os.path.join(dirname, save_path)
        checkpoint = torch.load(save_path)
        for k, v in self.saveables.items():
            v.load_state_dict(checkpoint[k].state_dict())

    def sample(self, m):
        """
        Sample a minibatch of outputs from the generator

        :param m (int): the number of samples to draw
        """
        # produce noise from the noise prior
        # NOTE TODO: for now this only accepts 1d input dim
        z_mb = self.noise_prior((m, self.input_dim))
        # produce output by calling the generator
        gen_mb = self(z_mb)

        return gen_mb

    @abstractmethod
    def __call__(self):
        """
        Need to be able to apply the generator to some noise to produce outputs
        """
        ...

    @abstractmethod
    def update(self):
        """
        Every generator must have a way to update its parameters
        """
        ...

    

class MLPGenerator(Generator):
    """
    This class is for an MLP Generator (i.e. feedforward neural network)
    Its output size is the same as the training data, and its input size is (probably) that size too.
    """

    def __init__(self, noise_prior, input_dim=1, output_dim=1, hidden_dim=10, n_hidden_layers=2, lr=1e-3
                 ):
        # initialise the module
        super(MLPGenerator, self).__init__(noise_prior, input_dim, output_dim)

        self.n_hidden_layers = n_hidden_layers
        self.hidden_dim = hidden_dim

        # store the layers
        self.hiddens = nn.ModuleList()
        self.batchnorms = nn.ModuleList()
        self.build_module()

        self.optim = torch.optim.Adam(params=self.parameters(), lr=lr)

    def build_module(self):
        # simulate a minibatch being passed through the network
        x = torch.zeros((2, self.input_dim))
        out = x
        self.input_layer = nn.Linear(self.input_dim, self.hidden_dim)
        out = self.input_layer(out)
        # we subtract one because the last hidden layer goes to the output
        for i in range(self.n_hidden_layers - 1):
            # calculate the dimension of the batchnorm needed
            batch_dim = out.size(-1)
            self.batchnorms.append(nn.BatchNorm1d(batch_dim))
            out = self.batchnorms[i](out)
            self.hiddens.append(nn.Linear(self.hidden_dim, self.hidden_dim))
            out = self.hiddens[i](out)
        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)

    def forward(self, X):
        out = X
        out = self.input_layer(out)
        out = F.relu(out)
        for i in range(self.n_hidden_layers - 1):
            out = self.batchnorms[i](out)
            out = self.hiddens[i](out)
            out = F.relu(out)
        out = self.output_layer(out)
        return out

    def update(self, m,  discriminator):
        """
        Update the generator based on the current discriminator 

        :param m (int): the size of the minibatch to train on
        :param discriminator (Discriminator): A discriminator subclass to make predictions
        :return loss (torch.Tensor): The generator loss from this training step
        """
        self.optim.zero_grad()
        gen_mb = self.sample(m)
        loss = self.compute_loss(gen_mb, discriminator)
        loss.backward()
        self.optim.step()
        return loss
