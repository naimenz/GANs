"""
Attempting to implement GANs, first for simple 1d probability distributions
and then ideally for MNIST.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import os

# trying out abstract base class stuff
from collections.abs import abstractmethod


class Discriminator(nn.Module):
    """
    This is a general template for discriminators.
    I don't have a lot of experience with this type of programming, but the idea is
    that all discriminators that I use will inherit from this, and this will have abstract methods.
    TODO: figure out if I should restructure this to CONTAIN an nn.Module rather than be one.
    """

    def __init__(self):
        """
        Constructor for Discriminator networks.
        Saving borrowed from RL CW again.

        :attr output_dim (int): Disciminator is always a binary classifier, so this is always 1
        :attr saveables(Dict[str, torch.nn.Module]): stuff to save
        """
        # initialise as a pytorch module
        super(Discriminator, self).__init__()

        self.output_dim = 1
        self.saveables = {}

    def compute_loss(self, train_mb, gen_mb):
        """Compute the loss for a minibatch.
        
        :param train_mb (torch.Tensor): minibatch of data from the training distribution, size (batch_size, (data_dims))
        :param gen_mb (torch.Tensor): minibatch of data generated by the generator, size (batch_size, (data_dims))
        :return loss (torch.Tensor): The scalar loss on this minibatch, from which to compute gradients
        """
        # make discriminator predictions for the two minibatches
        train_preds = self(train_mb)
        gen_preds = self(gen_mb)
        # return the loss
        return -torch.mean(torch.log(train_preds) + torch.log(1 - gen_preds))

    # borrowed from RL CW
    def save(self, path):
        """Saves saveable PyTorch models under given path

        The models will be saved in directory found under given path in file "models.pt"

        :param path (str): path to directory where to save models
        :return (str): path to file of saved models file
        """
        torch.save(self.saveables, path)
        return path

    # borrowed from RL CW
    def restore(self, save_path):
        """Restores PyTorch models from models file given by path

        :param save_path (str): path to file containing saved models
        """
        dirname, _ = os.path.split(os.path.abspath(__file__))
        save_path = os.path.join(dirname, save_path)
        checkpoint = torch.load(save_path)
        for k, v in self.saveables.items():
            v.load_state_dict(checkpoint[k].state_dict())

    @abstractmethod
    def update(self):
        """
        Every discriminator must have a way to update its parameters
        """
        ...


class MLPDiscriminator(Discriminator):
    """
    This class is for an MLP discriminator (feed-forward neural network).
    It inherits from the Disciminator base class
    """
    def __init__(self, input_dim=1, hidden_dim=10, n_hidden_layers=2, lr=1e-3
                 ):
        # initialise the module
        super(Discriminator, self).__init__()

        self.n_hidden_layers = n_hidden_layers
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # store the layers
        self.hiddens = nn.ModuleList()
        self.batchnorms = nn.ModuleList()
        self.build_module()

        # build an optimiser for this network's params
        self.optim = torch.optim.Adam(params=self.parameters())

    def build_module(self):
        # simulate a minibatch of size 2 being passed through the network
        x = torch.zeros((2, self.input_dim))
        out = x
        self.input_layer = nn.Linear(self.input_dim, self.hidden_dim)
        out = self.input_layer(out)
        # we subtract one because the last hidden layer goes to the output
        for i in range(self.n_hidden_layers - 1):
            # calculate the dimension of the batchnorm needed
            batch_dim = out.size(-1)
            self.batchnorms.append(nn.BatchNorm1d(batch_dim))
            out = self.batchnorms[i](out)
            self.hiddens.append(nn.Linear(self.hidden_dim, self.hidden_dim))
            out = self.hiddens[i](out)
        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)

    def forward(self, X):
        out = X
        out = self.input_layer(out)
        out = F.relu(out)
        for i in range(self.n_hidden_layers - 1):
            out = self.batchnorms[i](out)
            out = self.hiddens[i](out)
            out = F.relu(out)
        # on the last output, we apply a sigmoid to get a binary prediction
        out = self.output_layer(out)
        out = torch.sigmoid(out)
        return out

    def update(self, train_mb, gen_mb):
        """
        Update the network's parameters given a minibatch of training examples and generator samples

        :param train_mb (torch.Tensor): minibatch of data from the training distribution, size (batch_size, (data_dims))
        :param gen_mb (torch.Tensor): minibatch of data generated by the generator, size (batch_size, (data_dims))
        :return loss (torch.Tensor): The scalar loss on this minibatch
        """

        # first we zero the gradients, then compute the loss, backpropagate, and take a step 
        self.optim.zero_grad()
        loss = self.compute_loss(train_mb, gen_mb)
        loss.backward()
        self.optim.step()
        return loss

class GeneratorNetwork(nn.Module):
    """
    This class if for the discriminator.
    Its output size is the same as the training data, and its input size is (probably) that size too.
    """

    def __init__(self, input_dim=1, hidden_dim=10, output_dim=1, n_hidden_layers=2, lr=1e-3
                 ):
        # initialise the module
        super(GeneratorNetwork, self).__init__()

        self.n_hidden_layers = n_hidden_layers
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        # store the layers
        self.hiddens = nn.ModuleList()
        self.batchnorms = nn.ModuleList()
        self.build_module()

    def build_module(self):
        # simulate a minibatch being passed through the network
        x = torch.zeros((2, self.input_dim))
        out = x
        self.input_layer = nn.Linear(self.input_dim, self.hidden_dim)
        out = self.input_layer(out)
        # we subtract one because the last hidden layer goes to the output
        for i in range(self.n_hidden_layers - 1):
            # calculate the dimension of the batchnorm needed
            batch_dim = out.size(-1)
            self.batchnorms.append(nn.BatchNorm1d(batch_dim))
            out = self.batchnorms[i](out)
            self.hiddens.append(nn.Linear(self.hidden_dim, self.hidden_dim))
            out = self.hiddens[i](out)
        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)

    def forward(self, X):
        out = X
        out = self.input_layer(out)
        out = F.relu(out)
        for i in range(self.n_hidden_layers - 1):
            out = self.batchnorms[i](out)
            out = self.hiddens[i](out)
            out = F.relu(out)
        out = self.output_layer(out)
        return out
